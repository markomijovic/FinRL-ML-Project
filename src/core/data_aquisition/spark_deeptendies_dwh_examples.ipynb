{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG7CIYITM5O4"
      },
      "source": [
        "# Postgres Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FzoKjT0q5rO",
        "outputId": "272a08d1-3d09-43e2-e38b-58578de123dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SADeprecationWarning: Calling URL() directly is deprecated and will be disabled in a future release.  The public constructor for URL is now the URL.create() method.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ]
        }
      ],
      "source": [
        "# Auth\n",
        "from sqlalchemy.engine.url import URL\n",
        "from sqlalchemy import create_engine\n",
        "postgres_db = {'drivername': 'postgresql',\n",
        "               'username': 'postgres',\n",
        "               'password': 'XM8(eTha++ma<(@C',\n",
        "               'host': '34.71.18.220',\n",
        "               'port': 5432,\n",
        "               'database': 'deeptendies_sandbox'}\n",
        "pgURL = URL(**postgres_db)\n",
        "engine = create_engine(pgURL)\n",
        "\n",
        "import pandas as pd\n",
        "schema = 'finnhub_api_ingestion_stock_candles'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urDLb1ANNc4b"
      },
      "source": [
        "```python\n",
        "# show all schemas within a db\n",
        "query = f\"\"\"\n",
        "select schema_name\n",
        "from information_schema.schemata;\n",
        "\"\"\"\n",
        "pd.read_sql(query, con=engine)\n",
        "\n",
        "# select a specific table\n",
        "table_name = 'finnhub_api_ingestion_earning_calendar_earning_calendar'\n",
        "pd.read_sql(f\"SELECT * FROM \\\"{schema}\\\".\\\"{table_name}\\\"\",\n",
        "                    con=engine)\n",
        "\n",
        "# Show all tables within a schema\n",
        "query = f\"\"\"\n",
        "SELECT * FROM information_schema.tables \n",
        "WHERE table_schema = '{schema}' \n",
        "\"\"\"\n",
        "pd.read_sql(query, con=engine)[['table_catalog', 'table_schema', 'table_name']].to_dict()\n",
        "\n",
        "# select a specific table\n",
        "table_name = 'finnhub_api_ingestion_earning_calendar_earning_calendar'\n",
        "pd.read_sql(f\"SELECT * FROM \\\"{schema}\\\".\\\"{table_name}\\\"\",\n",
        "                    con=engine)\n",
        "\n",
        "# transformation\n",
        "from datetime import date\n",
        "today = date.today().strftime(\"%Y-%m-%d\")\n",
        "df = pd.read_sql(f\"SELECT * FROM \\\"{schema}\\\".\\\"{table_name}\\\"\",\n",
        "                    con=engine)\n",
        "df = df[df.date > today].sort_values(['date'],ascending = True).head(50).sort_values(['epsEstimate'],ascending = False)\n",
        "topic_name = 'finnhub_api_ingestion_earning_calendar_earning_calendar' + \"_sorted\"\n",
        "\n",
        "# to sql\n",
        "df.to_sql(name=topic_name,\n",
        "            con=engine,\n",
        "            schema=schema,\n",
        "            if_exists='append',\n",
        "            index=False,\n",
        "            method='multi')                    \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laKr5f9a0WVS"
      },
      "source": [
        "# BigQuery Examples\n",
        "https://colab.research.google.com/notebooks/bigquery.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwE-88N4Ng3B"
      },
      "source": [
        "```python\n",
        "# Auth\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "%load_ext google.colab.data_table\n",
        "\n",
        "project_id = 'deeptendies-platform'\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client(project=project_id)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrcHY0WqNsvX"
      },
      "source": [
        "read / write\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Write to BQ\n",
        "pd.io.gbq.to_gbq(\n",
        "    dataframe = df,\n",
        "    destination_table = \"deeptendies_dwh.finnhub_api_ingestion_earning_calendar_earning_calendar\",\n",
        "    project_id=project_id,\n",
        "    if_exists = \"replace\"\n",
        ")\n",
        "\n",
        "# Read from BQ\n",
        "df = pd.io.gbq.read_gbq(\"\"\"\n",
        "    SELECT * FROM `deeptendies-platform.deeptendies_dwh.finnhub_api_ingestion_earning_calendar_earning_calendar`\n",
        "    LIMIT 1000\n",
        "    \"\"\",\n",
        "    project_id=project_id,\n",
        "    dialect='standard')\n",
        "\n",
        "df.head()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCx14NwKW__a"
      },
      "source": [
        "# Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QH1i-kQSQcG",
        "outputId": "17080b5b-8a41-4320-96ec-e62b5549bf0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee_B9iP_OaxN"
      },
      "outputs": [],
      "source": [
        "ticker =\"TQQQ\"\n",
        "\n",
        "query = f\"\"\"\n",
        "select *\n",
        "from \\\"finnhub_api_ingestion_stock_candles\\\".\\\"{ticker}\\\";\n",
        "\"\"\"\n",
        "df = pd.read_sql(query, con=engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tFQVYeMHGUv"
      },
      "source": [
        "c: close, h: high, l: low, o: open, s: status, t: timestamp, v: volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL8LhQ4CVTw1",
        "outputId": "5c920993-0678-4aa9-86b5-925dcc389055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            " |-- Volume: long (nullable = true)\n",
            "\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o91.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (10.0.0.18 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-4-a796adeafdda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GOOGL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-4-a796adeafdda>\u001b[0m in \u001b[0;36mload_csv\u001b[1;34m(TICKER)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msparkDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msparkDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msparkDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_spark_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o91.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (10.0.0.18 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 29 more\r\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "def load_csv(TICKER: str):\n",
        "    df = pd.read_csv(\"data/\"+TICKER+\".csv\")\n",
        "    spark = create_spark_session()\n",
        "    sparkDF = spark.createDataFrame(df)\n",
        "    sparkDF.printSchema()\n",
        "    sparkDF.show(3)\n",
        "\n",
        "def create_spark_session():\n",
        "    spark = SparkSession.builder.master(\"local[1]\").appName(\"ENSF612Project\").getOrCreate()\n",
        "    return spark\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_csv(\"GOOGL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZptU5jNIr78"
      },
      "source": [
        "hl_movement: high low move(difference between the highest and lowest point);\n",
        "daily_movement: close value- open value;\n",
        "hlm_divv:hl_movement/volume\n",
        "daily_divv: daily_movement/volume\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "fi-wDPWiRJF9",
        "outputId": "39a54f92-bcb9-404f-d7dd-653062db3d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['c', 'h', 'l', 'o', 's', 't', 'v'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[c: double, h: double, l: double, o: double, s: string, t: bigint, v: bigint, date: string]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import LongType, StringType\n",
        "\n",
        "print(df.columns)\n",
        "\n",
        "@udf(\"string\")\n",
        "def get_date_udf(a):\n",
        "    val = str(datetime.datetime.fromtimestamp(a).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    return val\n",
        "\n",
        "@udf(\"double\")\n",
        "def hl_movement(h,l):\n",
        "    return h - l\n",
        "\n",
        "@udf(\"double\")\n",
        "def daily_movement(o,c):\n",
        "    return o - c\n",
        "\n",
        "@udf(\"double\")\n",
        "def hlm_divv(m,v):\n",
        "    return m /v\n",
        "\n",
        "@udf(\"double\")\n",
        "def dailym_divv(m,v):\n",
        "    return m /v\n",
        "\n",
        "display(sparkDF.select(\"*\", get_date_udf(\"t\").alias(\"date\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4q42rrUaIeu"
      },
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0qg58A9Wobn",
        "outputId": "c1e6ec83-fe43-4023-e408-9358984f41b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+--------+------+---+----------+--------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
            "|     c|       h|       l|     o|  s|         t|       v|               date|             hlmov|              daymov|           hlmovdivv|            dmovdivv|\n",
            "+------+--------+--------+------+---+----------+--------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
            "| 71.08|   79.19|   70.74| 77.84| ok|1604880000|82932764|2020-11-09 00:00:00| 8.450000000000003|   6.760000000000005|  0.1188801350590884| 0.09510410804727075|\n",
            "|67.245|  69.625|  65.175| 68.57| ok|1604966400|84333318|2020-11-10 00:00:00| 4.450000000000003|  1.3249999999999886| 0.06617592386051012| 0.01970406721689328|\n",
            "|71.765|  72.095|   68.74|69.035| ok|1605052800|63415370|2020-11-11 00:00:00| 3.355000000000004|  -2.730000000000004| 0.04674980840242463|-0.03804082770152587|\n",
            "|70.715|   73.46|   69.86|72.425| ok|1605139200|71928550|2020-11-12 00:00:00|3.5999999999999943|  1.7099999999999937|0.050908576681043545|0.024181573923495632|\n",
            "| 72.57|   73.04|   70.33| 71.89| ok|1605225600|52448626|2020-11-13 00:00:00| 2.710000000000008| -0.6799999999999926|0.037343254788480204|-0.00937026319415...|\n",
            "| 74.24|  74.505|  71.872|72.375| ok|1605484800|53437256|2020-11-16 00:00:00|2.6329999999999956| -1.8649999999999949|0.035466056034482704|-0.02512122844827...|\n",
            "|73.555|74.67995|  73.065|  74.4| ok|1605571200|45675904|2020-11-17 00:00:00|1.6149500000000074|  0.8449999999999989|  0.0219556794235607|   0.011488002175243|\n",
            "| 71.85|  74.375|   71.85| 73.13| ok|1605657600|38585312|2020-11-18 00:00:00|2.5250000000000057|  1.2800000000000011| 0.03514265831593606|0.017814892136395286|\n",
            "| 73.63|  73.845|   70.98|71.455| ok|1605744000|39835448|2020-11-19 00:00:00| 2.864999999999995|  -2.174999999999997|0.038910770066548896|-0.02953958984109...|\n",
            "|72.135|  74.135|   72.06|73.525| ok|1605830400|32498366|2020-11-20 00:00:00| 2.075000000000003|  1.3900000000000006| 0.02876550911485413| 0.01926942538296251|\n",
            "| 72.15|   73.87|   70.38| 72.91| ok|1606089600|44765202|2020-11-23 00:00:00| 3.490000000000009|  0.7599999999999909|0.048371448371448494|0.010533610533610406|\n",
            "|  75.1|  75.515|  71.535| 72.91| ok|1606176000|39003772|2020-11-24 00:00:00| 3.980000000000004| -2.1899999999999977| 0.05299600532623175|-0.02916111850865...|\n",
            "|76.515|  77.005|  75.225| 75.86| ok|1606262400|32169410|2020-11-25 00:00:00|1.7800000000000011| -0.6550000000000011|0.023263412402796852|-0.00856041299091...|\n",
            "|78.605|79.53495|  77.765| 78.02| ok|1606435200|23231878|2020-11-27 00:00:00|1.7699499999999944|  -0.585000000000008|0.022517015457031924|-0.00744227466446...|\n",
            "| 79.01|79.39005|   75.29| 78.86| ok|1606694400|46155528|2020-11-30 00:00:00| 4.100049999999996|-0.15000000000000568| 0.05189279837995185|-0.00189849386153...|\n",
            "|82.115|  83.475|80.05005|80.895| ok|1606780800|42698904|2020-12-01 00:00:00|3.4249499999999955| -1.2199999999999989| 0.04170918833343477|-0.01485721244595...|\n",
            "|82.445|82.72065|   79.59|81.105| ok|1606867200|36532328|2020-12-02 00:00:00| 3.130650000000003| -1.3399999999999892| 0.03797258778579663|-0.01625325974892...|\n",
            "| 82.78|  84.095| 82.1755|82.645| ok|1606953600|39365414|2020-12-03 00:00:00|1.9194999999999993|-0.13500000000000512|0.023187968108238696|-0.00163082870258...|\n",
            "|83.795|   83.92|  82.255|  82.8| ok|1607040000|31772014|2020-12-04 00:00:00|1.6650000000000063| -0.9950000000000045| 0.01986992063965638|-0.01187421683871...|\n",
            "|85.215|   85.51|    83.8| 83.95| ok|1607299200|31006936|2020-12-07 00:00:00| 1.710000000000008| -1.2650000000000006|0.020066889632107118|-0.01484480431848...|\n",
            "+------+--------+--------+------+---+----------+--------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sdf = sparkDF.select(\"*\",\n",
        "                     get_date_udf(\"t\").alias(\"date\"),\n",
        "                     hl_movement(\"h\", \"l\").alias(\"hlmov\"),\n",
        "                     daily_movement(\"o\",\"c\").alias(\"daymov\"),\n",
        "                     ).sort(\"date\")\n",
        "sdf = sdf.select(\"*\", hlm_divv(\"hlmov\",\"c\").alias(\"hlmovdivv\"),\n",
        "                 dailym_divv(\"daymov\",\"c\").alias(\"dmovdivv\")\n",
        "                 )\n",
        "sdf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqeyoENK_CAX"
      },
      "outputs": [],
      "source": [
        "# converting back to pandas\n",
        "df = sdf.toPandas().dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STWUnJPeFrUK"
      },
      "outputs": [],
      "source": [
        "df[['dc', 'dh', 'dl','do', 'dv']]=df[[ 'c','h','l','o','v']].diff()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "SjqQQ41tGDat",
        "outputId": "ce5050e5-4132-4032-a145-77eb8b52d7c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>h</th>\n",
              "      <th>l</th>\n",
              "      <th>o</th>\n",
              "      <th>s</th>\n",
              "      <th>t</th>\n",
              "      <th>v</th>\n",
              "      <th>date</th>\n",
              "      <th>hlmov</th>\n",
              "      <th>daymov</th>\n",
              "      <th>hlmovdivv</th>\n",
              "      <th>dmovdivv</th>\n",
              "      <th>dc</th>\n",
              "      <th>dh</th>\n",
              "      <th>dl</th>\n",
              "      <th>do</th>\n",
              "      <th>dv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71.080</td>\n",
              "      <td>79.1900</td>\n",
              "      <td>70.740</td>\n",
              "      <td>77.840</td>\n",
              "      <td>ok</td>\n",
              "      <td>1604880000</td>\n",
              "      <td>82932764</td>\n",
              "      <td>2020-11-09</td>\n",
              "      <td>8.4500</td>\n",
              "      <td>6.760</td>\n",
              "      <td>0.118880</td>\n",
              "      <td>0.095104</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>67.245</td>\n",
              "      <td>69.6250</td>\n",
              "      <td>65.175</td>\n",
              "      <td>68.570</td>\n",
              "      <td>ok</td>\n",
              "      <td>1604966400</td>\n",
              "      <td>84333318</td>\n",
              "      <td>2020-11-10</td>\n",
              "      <td>4.4500</td>\n",
              "      <td>1.325</td>\n",
              "      <td>0.066176</td>\n",
              "      <td>0.019704</td>\n",
              "      <td>-3.835</td>\n",
              "      <td>-9.5650</td>\n",
              "      <td>-5.565</td>\n",
              "      <td>-9.270</td>\n",
              "      <td>1400554.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>71.765</td>\n",
              "      <td>72.0950</td>\n",
              "      <td>68.740</td>\n",
              "      <td>69.035</td>\n",
              "      <td>ok</td>\n",
              "      <td>1605052800</td>\n",
              "      <td>63415370</td>\n",
              "      <td>2020-11-11</td>\n",
              "      <td>3.3550</td>\n",
              "      <td>-2.730</td>\n",
              "      <td>0.046750</td>\n",
              "      <td>-0.038041</td>\n",
              "      <td>4.520</td>\n",
              "      <td>2.4700</td>\n",
              "      <td>3.565</td>\n",
              "      <td>0.465</td>\n",
              "      <td>-20917948.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70.715</td>\n",
              "      <td>73.4600</td>\n",
              "      <td>69.860</td>\n",
              "      <td>72.425</td>\n",
              "      <td>ok</td>\n",
              "      <td>1605139200</td>\n",
              "      <td>71928550</td>\n",
              "      <td>2020-11-12</td>\n",
              "      <td>3.6000</td>\n",
              "      <td>1.710</td>\n",
              "      <td>0.050909</td>\n",
              "      <td>0.024182</td>\n",
              "      <td>-1.050</td>\n",
              "      <td>1.3650</td>\n",
              "      <td>1.120</td>\n",
              "      <td>3.390</td>\n",
              "      <td>8513180.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>72.570</td>\n",
              "      <td>73.0400</td>\n",
              "      <td>70.330</td>\n",
              "      <td>71.890</td>\n",
              "      <td>ok</td>\n",
              "      <td>1605225600</td>\n",
              "      <td>52448626</td>\n",
              "      <td>2020-11-13</td>\n",
              "      <td>2.7100</td>\n",
              "      <td>-0.680</td>\n",
              "      <td>0.037343</td>\n",
              "      <td>-0.009370</td>\n",
              "      <td>1.855</td>\n",
              "      <td>-0.4200</td>\n",
              "      <td>0.470</td>\n",
              "      <td>-0.535</td>\n",
              "      <td>-19479924.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>158.780</td>\n",
              "      <td>159.3700</td>\n",
              "      <td>155.960</td>\n",
              "      <td>156.025</td>\n",
              "      <td>ok</td>\n",
              "      <td>1635811200</td>\n",
              "      <td>30042809</td>\n",
              "      <td>2021-11-02</td>\n",
              "      <td>3.4100</td>\n",
              "      <td>-2.755</td>\n",
              "      <td>0.021476</td>\n",
              "      <td>-0.017351</td>\n",
              "      <td>2.090</td>\n",
              "      <td>2.4400</td>\n",
              "      <td>2.780</td>\n",
              "      <td>0.225</td>\n",
              "      <td>-2468558.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>163.820</td>\n",
              "      <td>164.3200</td>\n",
              "      <td>158.070</td>\n",
              "      <td>159.340</td>\n",
              "      <td>ok</td>\n",
              "      <td>1635897600</td>\n",
              "      <td>36302613</td>\n",
              "      <td>2021-11-03</td>\n",
              "      <td>6.2500</td>\n",
              "      <td>-4.480</td>\n",
              "      <td>0.038152</td>\n",
              "      <td>-0.027347</td>\n",
              "      <td>5.040</td>\n",
              "      <td>4.9500</td>\n",
              "      <td>2.110</td>\n",
              "      <td>3.315</td>\n",
              "      <td>6259804.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>169.930</td>\n",
              "      <td>171.3477</td>\n",
              "      <td>164.730</td>\n",
              "      <td>165.430</td>\n",
              "      <td>ok</td>\n",
              "      <td>1635984000</td>\n",
              "      <td>40219862</td>\n",
              "      <td>2021-11-04</td>\n",
              "      <td>6.6177</td>\n",
              "      <td>-4.500</td>\n",
              "      <td>0.038944</td>\n",
              "      <td>-0.026481</td>\n",
              "      <td>6.110</td>\n",
              "      <td>7.0277</td>\n",
              "      <td>6.660</td>\n",
              "      <td>6.090</td>\n",
              "      <td>3917249.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>170.560</td>\n",
              "      <td>173.6600</td>\n",
              "      <td>168.730</td>\n",
              "      <td>172.460</td>\n",
              "      <td>ok</td>\n",
              "      <td>1636070400</td>\n",
              "      <td>42626357</td>\n",
              "      <td>2021-11-05</td>\n",
              "      <td>4.9300</td>\n",
              "      <td>1.900</td>\n",
              "      <td>0.028905</td>\n",
              "      <td>0.011140</td>\n",
              "      <td>0.630</td>\n",
              "      <td>2.3123</td>\n",
              "      <td>4.000</td>\n",
              "      <td>7.030</td>\n",
              "      <td>2406495.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>169.860</td>\n",
              "      <td>171.9000</td>\n",
              "      <td>169.060</td>\n",
              "      <td>170.500</td>\n",
              "      <td>ok</td>\n",
              "      <td>1636329600</td>\n",
              "      <td>28229913</td>\n",
              "      <td>2021-11-08</td>\n",
              "      <td>2.8400</td>\n",
              "      <td>0.640</td>\n",
              "      <td>0.016720</td>\n",
              "      <td>0.003768</td>\n",
              "      <td>-0.700</td>\n",
              "      <td>-1.7600</td>\n",
              "      <td>0.330</td>\n",
              "      <td>-1.960</td>\n",
              "      <td>-14396444.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>252 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           c         h        l        o  ...      dh     dl     do          dv\n",
              "0     71.080   79.1900   70.740   77.840  ...     NaN    NaN    NaN         NaN\n",
              "1     67.245   69.6250   65.175   68.570  ... -9.5650 -5.565 -9.270   1400554.0\n",
              "2     71.765   72.0950   68.740   69.035  ...  2.4700  3.565  0.465 -20917948.0\n",
              "3     70.715   73.4600   69.860   72.425  ...  1.3650  1.120  3.390   8513180.0\n",
              "4     72.570   73.0400   70.330   71.890  ... -0.4200  0.470 -0.535 -19479924.0\n",
              "..       ...       ...      ...      ...  ...     ...    ...    ...         ...\n",
              "247  158.780  159.3700  155.960  156.025  ...  2.4400  2.780  0.225  -2468558.0\n",
              "248  163.820  164.3200  158.070  159.340  ...  4.9500  2.110  3.315   6259804.0\n",
              "249  169.930  171.3477  164.730  165.430  ...  7.0277  6.660  6.090   3917249.0\n",
              "250  170.560  173.6600  168.730  172.460  ...  2.3123  4.000  7.030   2406495.0\n",
              "251  169.860  171.9000  169.060  170.500  ... -1.7600  0.330 -1.960 -14396444.0\n",
              "\n",
              "[252 rows x 17 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['date'] = pd.to_datetime(df[\"date\"]).dt.date\n",
        "df = df [df['date'] < datetime.date.today()]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE0SpLYAHOns",
        "outputId": "af9f38ab-3801-4990-8046-9072cb754c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autokeras in /usr/local/lib/python3.7/dist-packages (1.0.16.post1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from autokeras) (21.2)\n",
            "Requirement already satisfied: keras-tuner<1.1,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.0.4)\n",
            "Requirement already satisfied: tensorflow<2.6,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from autokeras) (2.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autokeras) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (2.7.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (1.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (1.4.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.1,>=1.0.2->autokeras) (2.23.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (3.1.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.34.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.12)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (0.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (0.37.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.3.0->autokeras) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6,>=2.3.0->autokeras) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner<1.1,>=1.0.2->autokeras) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner<1.1,>=1.0.2->autokeras) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner<1.1,>=1.0.2->autokeras) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner<1.1,>=1.0.2->autokeras) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner<1.1,>=1.0.2->autokeras) (3.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.2.5)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->autokeras) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner<1.1,>=1.0.2->autokeras) (0.7.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install autokeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqWMWJPKHQq6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import autokeras as ak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LdJ6Fc3Hf6E",
        "outputId": "ea1347a4-01bb-4fb9-806e-039141c9c205"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['c', 'h', 'l', 'o', 's', 't', 'v', 'date', 'hlmov', 'daymov',\n",
              "       'hlmovdivv', 'dmovdivv', 'dc', 'dh', 'dl', 'do', 'dv'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUI4ErWaHTFz",
        "outputId": "931313c8-f01f-4f8f-b1d5-9bb30d31467e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(175, 7)\n",
            "(175, 1)\n"
          ]
        }
      ],
      "source": [
        "# We're using deep learning algorithm, LSTM, to compare with the algorithms from finRL library \n",
        "# Using autokeras timeseries forecaster\n",
        "# https://autokeras.com/tutorial/timeseries_forecaster/\n",
        "dataset = df.dropna()\n",
        "dataset = dataset.replace(\",\", \".\", regex=True)\n",
        "\n",
        "val_split = int(len(dataset) * 0.7)\n",
        "data_train = dataset[:val_split]\n",
        "validation_data = dataset[val_split:]\n",
        "\n",
        "x_cols =     ['v', 'hlmov', 'daymov',\n",
        "       'dc', \n",
        "    #    'dh',\n",
        "        'dl', 'do', 'dv']\n",
        "\n",
        "ycols = ['dh']\n",
        "\n",
        "data_x = data_train[\n",
        "    x_cols\n",
        "].astype(\"float64\")\n",
        "\n",
        "data_x_val = validation_data[\n",
        "    x_cols\n",
        "].astype(\"float64\")\n",
        "\n",
        "# Data with train data and the unseen data from subsequent time steps.\n",
        "data_x_test = dataset[\n",
        "    x_cols\n",
        "].astype(\"float64\")\n",
        "\n",
        "data_y = data_train[ycols].astype(\"float64\")\n",
        "\n",
        "data_y_val = validation_data[ycols].astype(\"float64\")\n",
        "\n",
        "print(data_x.shape)  # (6549, 12)\n",
        "print(data_y.shape)  # (6549,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZukiqWpfIYQU",
        "outputId": "b85992fa-4390-4163-c555-336c1870d55e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 15 Complete [00h 07m 00s]\n",
            "val_loss: 7.765697002410889\n",
            "\n",
            "Best val_loss So Far: 6.567526340484619\n",
            "Total elapsed time: 00h 20m 39s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./time_series_forecaster/best_model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./time_series_forecaster/best_model/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ]
        }
      ],
      "source": [
        "predict_from = 1\n",
        "predict_until = 10\n",
        "lookback = 10\n",
        "clf = ak.TimeseriesForecaster(\n",
        "    lookback=lookback,\n",
        "    predict_from=predict_from,\n",
        "    predict_until=predict_until,\n",
        "    # max_trials=1,\n",
        "    objective=\"val_loss\",\n",
        ")\n",
        "# Train the TimeSeriesForecaster with train data\n",
        "clf.fit(\n",
        "    x=data_x,\n",
        "    y=data_y,\n",
        "    validation_data=(data_x_val, data_y_val),\n",
        "    batch_size=32,\n",
        "    # epochs=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m37eYyrMLohW",
        "outputId": "80732820-2694-4c01-b3d3-9a0d3fba50f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 13ms/step\n",
            "(10, 1)\n",
            "3/3 [==============================] - 2s 9ms/step - loss: 6.5675 - mean_squared_error: 6.5675\n",
            "[6.567526340484619, 6.567526340484619]\n",
            "next 10 forecast dh:\n",
            "[[ 0.8707507 ]\n",
            " [ 1.0773495 ]\n",
            " [ 0.43577257]\n",
            " [ 1.01952   ]\n",
            " [ 1.2325165 ]\n",
            " [ 1.2308419 ]\n",
            " [ 0.4431724 ]\n",
            " [-0.44880378]\n",
            " [ 0.5153476 ]\n",
            " [-0.4437851 ]]\n"
          ]
        }
      ],
      "source": [
        "# Predict with the best model(includes original training data).\n",
        "predictions = clf.predict(data_x_test)\n",
        "print(predictions.shape)\n",
        "# Evaluate the best model with testing data.\n",
        "print(clf.evaluate(data_x_val, data_y_val))\n",
        "print(\"next 10 forecast dh:\")\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2NVwPhufi4c"
      },
      "source": [
        "# dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrBF98pwffBK",
        "outputId": "f1d19837-669e-4fa5-d857-18ed9dbc4a39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(175, 7)\n",
            "(175, 1)\n"
          ]
        }
      ],
      "source": [
        "# Using autokeras timeseries forecaster\n",
        "# https://autokeras.com/tutorial/timeseries_forecaster/\n",
        "dataset = df.dropna()\n",
        "dataset = dataset.replace(\",\", \".\", regex=True)\n",
        "\n",
        "val_split = int(len(dataset) * 0.7)\n",
        "data_train = dataset[:val_split]\n",
        "validation_data = dataset[val_split:]\n",
        "\n",
        "x_cols =     ['v', 'hlmov', 'daymov',\n",
        "       'dc', \n",
        "       'dh',\n",
        "        # 'dl',\n",
        "         'do', 'dv']\n",
        "\n",
        "ycols = ['dl']\n",
        "\n",
        "data_x = data_train[\n",
        "    x_cols\n",
        "].astype(\"float64\")\n",
        "\n",
        "data_x_val = validation_data[\n",
        "    x_cols\n",
        "].astype(\"float64\")\n",
        "\n",
        "# Data with train data and the unseen data from subsequent time steps.\n",
        "data_x_test = dataset[\n",
        "    x_cols\n",
        "].astype(\"float64\")\n",
        "\n",
        "data_y = data_train[ycols].astype(\"float64\")\n",
        "\n",
        "data_y_val = validation_data[ycols].astype(\"float64\")\n",
        "\n",
        "print(data_x.shape)  # (6549, 12)\n",
        "print(data_y.shape)  # (6549,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9feBhkm-fs0C",
        "outputId": "ef7a4fda-43cf-4410-e3f5-4f3eed1053c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./time_series_forecaster/oracle.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./time_series_forecaster/oracle.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Tuner from ./time_series_forecaster/tuner0.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Tuner from ./time_series_forecaster/tuner0.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./time_series_forecaster/best_model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./time_series_forecaster/best_model/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ]
        }
      ],
      "source": [
        "predict_from = 1\n",
        "predict_until = 10\n",
        "lookback = 10\n",
        "clf = ak.TimeseriesForecaster(\n",
        "    lookback=lookback,\n",
        "    predict_from=predict_from,\n",
        "    predict_until=predict_until,\n",
        "    # max_trials=1,\n",
        "    objective=\"val_loss\",\n",
        ")\n",
        "# Train the TimeSeriesForecaster with train data\n",
        "clf.fit(\n",
        "    x=data_x,\n",
        "    y=data_y,\n",
        "    validation_data=(data_x_val, data_y_val),\n",
        "    batch_size=32,\n",
        "    # epochs=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67XS5ZyFfuxn",
        "outputId": "63db3b3b-c3d5-49bf-8ab3-f3657d85ece3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 9ms/step\n",
            "(10, 1)\n",
            "3/3 [==============================] - 2s 9ms/step - loss: 11.1515 - mean_squared_error: 11.1515\n",
            "[11.151470184326172, 11.151470184326172]\n",
            "next 10 forecast dl:\n",
            "[[ 0.8707507 ]\n",
            " [ 1.0773495 ]\n",
            " [ 0.43577257]\n",
            " [ 1.01952   ]\n",
            " [ 1.2325165 ]\n",
            " [ 1.2308419 ]\n",
            " [ 0.4431724 ]\n",
            " [-0.44880378]\n",
            " [ 0.5153476 ]\n",
            " [-0.4437851 ]]\n"
          ]
        }
      ],
      "source": [
        "# Predict with the best model(includes original training data).\n",
        "predictions = clf.predict(data_x_test)\n",
        "print(predictions.shape)\n",
        "# Evaluate the best model with testing data.\n",
        "print(clf.evaluate(data_x_val, data_y_val))\n",
        "print(\"next 10 forecast dl:\")\n",
        "print(predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZG7CIYITM5O4",
        "laKr5f9a0WVS"
      ],
      "name": "spark_deeptendies_dwh_examples.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
